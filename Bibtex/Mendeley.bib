@article{Sarvotham2001,
abstract = {Most network traffic analysis and modeling studies lump all connections together into a single flow. Such aggregate traffic typically exhibits long-range-dependent (LRD) correlations and non-Gaussian marginal distributions. Importantly, in a typical aggregate traffic model, traffic bursts arise from many connections being active simultaneously. In this report, we develop a new framework for analyzing and modeling network traffic that moves beyond aggregation by incorporating connection-level information. A careful study of many traffic traces acquired in different networking situations reveals (in opposition to the aggregate modeling ideal) that traffic bursts typically arise from a single high-volume connection that dominates all others. We term such dominating connections alpha traffic. Alpha traffic is caused by large files transmissions over high bandwidth links and is extremely bursty (non-Gaussian). Stripping the alpha traffic from an aggregate trace leaves a beta traffic residual that is Gaussian, LRD, and shares the same fractal scaling exponent as the aggregate traffic. Beta traffic is caused by both small file transmissions and large files transmissions over low bandwidth links. In our alpha/beta traffic model, the heterogeneity of the network resources give rise to burstiness and heavy-tailed connection durations give rise to LRD.},
author = {Sarvotham, Shriram and Riedi, Rudolf and Baraniuk, Richard},
file = {:home/wcravens/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarvotham, Riedi, Baraniuk - 2001 - Connection-level Analysis and Modeling of Network Traffic.pdf:pdf},
title = {{Connection-level Analysis and Modeling of Network Traffic}},
year = {2001}
}
@article{Science2013,
author = {Science, Management},
number = {8},
pages = {947--954},
title = {{A MODEL FOR PREDICTING FREQUENCIES OF RANDOM EVENTS *}},
url = {https://www.jstor.org/stable/2632170},
volume = {33},
year = {2013}
}
@article{Krispin2019,
abstract = {The decomposition of time series This book introduces you to time series analysis and forecasting with R; this is one of the key fields in statistical programming and includes techniques for analyzing data to extract meaningful insights. You will explore methods, such as prediction with time series analysis, and identify the relationship between each data point in the series. Cover; Title Page; Copyright and Credits; Dedication; About Packt; Contributors; Table of Contents; Preface; Chapter 1: Introduction to Time Series Analysis and R; Technical requirements; Time series data; Historical background of time series analysis; Time series analysis; Learning with real-life examples; Getting started with R; Installing R; A brief introduction to R; R operators; Assignment operators; Arithmetic operators; Logical operators; Relational operators; The R package; Installation and maintenance of a package; Loading a package in the R working environment; The key packages VariablesImporting and loading data to R; Flat files; Web API; R datasets; Working and manipulating data; Querying the data; Help and additional resources; Summary; Chapter 2: Working with Date and Time Objects; Technical requirements; The date and time formats; Date and time objects in R; Creating date and time objects; Importing date and time objects; Reformatting and converting date objects; Handling numeric date objects; Reformatting and conversion of time objects; Time zone setting; Creating a date or time index; Manipulation of date and time with the lubridate package Reformatting date and time objects -- the lubridate wayUtility functions for date and time objects; Summary; Chapter 3: The Time Series Object; Technical requirement; The Natural Gas Consumption dataset; The attributes of the ts class; Multivariate time series objects; Creating a ts object; Creating an mts object; Setting the series frequency; Data manipulation of ts objects; The window function; Aggregating ts objects; Creating lags and leads for ts objects; Visualizing ts and mts objects; The plot.ts function; The dygraphs package; The TSstudio package; Summary Chapter 4: Working with zoo and xts ObjectsTechnical requirement; The zoo class; The zoo class attributes; The index of the zoo object; Working with date and time objects; Creating a zoo object; Working with multiple time series objects; The xts class; The xts class attributes; The xts functionality; The periodicity function; Manipulating the object index; Subsetting an xts object based on the index properties; Manipulating the zoo and xts objects; Merging time series objects; Rolling windows; Creating lags; Aggregating the zoo and xts objects; Plotting zoo and xts objects The plot.zoo functionThe plot.xts function; xts, zoo, or ts -- which one to use?; Summary; Chapter 5: Decomposition of Time Series Data; Technical requirement; The moving average function; The rolling window structure; The average method; The MA attributes; The simple moving average; Two-sided MA; A simple MA versus a two-sided MA; The time series components; The cycle component; The trend component; The seasonal component; The seasonal component versus the cycle component; White noise; The irregular component; The additive versus the multiplicative model; Handling multiplicative series},
author = {Krispin, Rami.},
isbn = {9781788624046},
pages = {438},
publisher = {Packt Publishing, Limited},
title = {{Hands-On Time Series Analysis with R : Perform Time Series Analysis and Forecasting Using R.}},
year = {2019}
}
@article{Mount2019,
abstract = {Description based upon print version of record. Part 2. Modeling methods. Intro -- Practical Data Science with R, Second Edition -- Nina Zumel and John Mount -- Copyright -- Dedication -- Brief Table of Contents -- Table of Contents -- Praise for the First Edition -- front matter -- Foreword -- Preface -- Acknowledgments -- About This Book -- What is data science? -- Roadmap -- Audience -- What is not in this book? -- Code conventions and downloads -- Working with this book -- Downloading the book's supporting materials/repository -- Book forum -- About the Authors -- About the Foreword Authors -- About the Cover Illustration -- Part 1. Introduction to data science Chapter 1. The data science process -- 1.1. The roles in a data science project -- 1.1.1. Project roles -- 1.2. Stages of a data science project -- 1.2.1. Defining the goal -- 1.2.2. Data collection and management -- 1.2.3. Modeling -- 1.2.4. Model evaluation and critique -- 1.2.5. Presentation and documentation -- 1.2.6. Model deployment and maintenance -- 1.3. Setting expectations -- 1.3.1. Determining lower bounds on model performance -- Summary -- Chapter 2. Starting with R and data -- 2.1. Starting with R -- 2.1.1. Installing R, tools, and examples -- 2.1.2. R programming 2.2. Working with data from files -- 2.2.1. Working with well-structured data from files or URLs -- 2.2.2. Using R with less-structured data -- 2.3. Working with relational databases -- 2.3.1. A production-size example -- Summary -- Chapter 3. Exploring data -- 3.1. Using summary statistics to spot problems -- 3.1.1. Typical problems revealed by data summaries -- 3.2. Spotting problems using graphics and visualization -- 3.2.1. Visually checking distributions for a single variable -- 3.2.2. Visually checking relationships between two variables -- Summary -- Chapter 4. Managing data 4.1. Cleaning data -- 4.1.1. Domain-specific data cleaning -- 4.1.2. Treating missing values -- 4.1.3. The vtreat package for automatically treating missing variables -- 4.2. Data transformations -- 4.2.1. Normalization -- 4.2.2. Centering and scaling -- 4.2.3. Log transformations for skewed and wide distributions -- 4.3. Sampling for modeling and validation -- 4.3.1. Test and training splits -- 4.3.2. Creating a sample group column -- 4.3.3. Record grouping -- 4.3.4. Data provenance -- Summary -- Chapter 5. Data engineering and data shaping -- 5.1. Data selection 5.1.1. Subsetting rows and columns -- 5.1.2. Removing records with incomplete data -- 5.1.3. Ordering rows -- 5.2. Basic data transforms -- 5.2.1. Adding new columns -- 5.2.2. Other simple operations -- 5.3. Aggregating transforms -- 5.3.1. Combining many rows into summary rows -- 5.4. Multitable data transforms -- 5.4.1. Combining two or more ordered data frames quickly -- 5.4.2. Principal methods to combine data from multiple tables -- 5.5. Reshaping transforms -- 5.5.1. Moving data from wide to tall form -- 5.5.2. Moving data from tall to wide form -- 5.5.3. Data coordinates -- Summary},
author = {Mount, John. and Zumel, Nina.},
isbn = {9781638352747},
publisher = {Manning Publications Co. LLC},
title = {{Practical Data Science with R}},
year = {2019}
}
@article{KrzanowskiVer2006,
author = {{Krzanowski Ver}, R},
file = {:home/wcravens/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krzanowski Ver - 2006 - Burst (of packets) and Burstiness.pdf:pdf},
title = {{Burst (of packets) and Burstiness}},
year = {2006}
}
@article{Oudah2019,
abstract = {Network traffic classification is a vital task for service operators, network engineers, and security specialists to manage network traffic, design networks, and detect threats. Identifying the type/name of applications that generate traffic is a challenging task as encrypting traffic becomes the norm for Internet communication. Therefore, relying on conventional techniques such as deep packet inspection (DPI) or port numbers is not efficient anymore. This paper proposes a novel flow statistical-based set of features that may be used for classifying applications by leveraging machine learning algorithms to yield high accuracy in identifying the type of applications that generate the traffic. The proposed features compute different timings between packets and flows. This work utilises tcptrace to extract features based on traffic burstiness and periods of inactivity (idle time) for the analysed traffic, followed by the C5.0 algorithm for determining the applications that generated it. The evaluation tests performed on a set of real, uncontrolled traffic, indicated that the method has an accuracy of 79% in identifying the correct network application.},
author = {Oudah, Hussein and Ghita, Bogdan and Bakhshi, Taimur and Alruban, Abdulrahman and Walker, David J.},
doi = {10.1155/2019/5758437},
file = {:home/wcravens/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oudah et al. - 2019 - Using Burstiness for Network Applications Classification.pdf:pdf},
issn = {2090715X},
journal = {Journal of Computer Networks and Communications},
publisher = {Hindawi Limited},
title = {{Using Burstiness for Network Applications Classification}},
volume = {2019},
year = {2019}
}
@article{Cheysson,
abstract = {In this paper, we study the time series generated by the event counts of the stationary Hawkes process. Using the cluster properties of the stationary Hawkes process, we prove an upper bound for its strong-mixing coefficient, and for its count series', provided that the reproduction kernel has a finite (1 + $\beta$)-th order moment (for a $\beta$ > 0). When the exact locations of points are not observed, but only counts over fixed time intervals, we propose a spectral approach to the estimation of Hawkes processes, based on Whittle's likelihood. This approach provides consistent and asymptotically normal estimates provided common regularity conditions on the reproduction kernel. Simulated datasets illustrate the performances of the estimation, notably, of the Hawkes reproduction mean and kernel, even with relatively large time intervals.},
author = {Cheysson, Felix and Lang, Gabriel},
file = {:home/wcravens/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheysson, Lang - Unknown - Strong-mixing rates for Hawkes processes and application to Whittle estimation from count data.pdf:pdf},
keywords = {Bartlett spectrum,Hawkes process,Strong mixing,Time series,Whittle estimation},
title = {{Strong-mixing rates for Hawkes processes and application to Whittle estimation from count data}},
url = {https://hal.archives-ouvertes.fr/hal-03117924}
}
@article{Hanke2021,
abstract = {Decentralized research data management (dRDM) systems handle digital research objects across participating nodes without critically relying on central services. We present four perspectives in defense of dRDM, illustrating that, in contrast to centralized or federated research data management solutions, a dRDM system based on heterogeneous but interoperable components can offer a sustainable, resilient, inclusive, and adaptive infrastructure for scientific stakeholders: An individual scientist or laboratory, a research institute, a domain data archive or cloud computing platform, and a collaborative multisite consortium. All perspectives share the use of a common, self-contained, portable data structure as an abstraction from current technology and service choices. In conjunction, the four perspectives review how varying requirements of independent scientific stakeholders can be addressed by a scalable, uniform dRDM solution and present a working system as an exemplary implementation.},
author = {Hanke, Michael and Pestilli, Franco and Wagner, Adina S. and Markiewicz, Christopher J. and Poline, Jean Baptiste and Halchenko, Yaroslav O.},
doi = {10.1515/NF-2020-0037/HTML},
file = {:home/wcravens/Downloads/10.1515_nf-2020-0037.pdf:pdf},
journal = {Neuroforum},
keywords = {BrainLife,Canadian open neuroscience platform,DataLad,Interoperability,OpenNeuro},
month = {feb},
number = {1},
pages = {17--25},
publisher = {De Gruyter Open Ltd},
title = {{In defense of decentralized research data management}},
volume = {27},
year = {2021}
}
