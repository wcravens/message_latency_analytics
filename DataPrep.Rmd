---
title: "Data Prep"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r include=FALSE} 
# Other Modules we may want later ggplot2, randtests, eventInterval
library( tidyverse )
library( tibble )
library( dplyr )
# library( rio )
library( bit64 )
library( psych )
```

## Import message data from csv source.

```{r msgw_in_import}
.msgw_in_import <- rio::import( "./Data/eqfa-msgw-in-0709.txt", format="csv" )
```

```{r import_data_is_monotonic}
is_monotonic <- all(.msgw_in_import$ts_ns == cummax(.msgw_in_import$ts_ns) )
if( !is_monotonic ) stop("Input data is not monotonic on timestamp, ts_ns." )
print( "The MSGW In data is monotonic with regard to event timestamp.")
```

```{r quick_view_of_raw_import_data, echo=FALSE, eval=FALSE }
.msgw_in_import
```

# Prepare input data for down stream analysis

1. Limit data set to a couple of minutes to speed up initial analytics.

2. Remove "PartyDetailsDefinitionRequest" Messages (they all have duplicate arrival times _and_ `null` processing time)

3. Normalize event `arrival_T` at $T_0 = 0$ Method: $T_i = T_i - T_0 \mbox{ for } i=0,...,n$.

4. Convert latency/processing time `PNQM Latency (ms)` to `duration` in nanoseconds

5. Use `Msg Type` as new factor `type`

6. Identify and label events with duplicate time-stamps.  Add a group total count and each individual event's number in the duplicated group to the data.

7. Number all events sequentially starting at 1 and incrementing by 1 for each new event arrival.  Store the individual event number in `N`

8. Spread out messages with duplicate arrival times by altering the arrival timestamp; Method: $T_i = T_i + k \mbox{ for } i = 0,...,n \mbox{ for } k=0..d$ where $d = $ number of duplicates at $T_i$ 

We can find the specifics later.  But I did look at the impact of this and
noticed that the amount of 'adjustment' necessary was considerably smaller
than the next 'smallest' gap between events.  An other method would have 
been to increase the precision of our number and using the least significant 
part to spread the message out in fraction of time about the duplicated 
arrival time.  The first option was considered much more reasonable and
easier to ensure we get correct.
    
9. Calculate inter-message `delta_t` in nanoseconds

10. Calculate event stop time, `departure_T`, in nanoseconds

11. Build the new data table out of the `N`, `arrival_t`, `delta_t`, `type`, `duration`, and `departure_T` fields.


```{r prepare_event_data}
time_limit <- 2 * 60 * 1e9 # Two minutes in ns

event_data <- .msgw_in_import %>%
 
  { if( time_limit ) filter(  ., ts_ns < first(ts_ns) + time_limit ) } %>% #  1.
  
  filter( !`Msg Type` %in% c( "PartyDetailsDefinitionRequest" ) ) %>% #       2.
  
  mutate(
    t = ts_ns - first(ts_ns),  #                                              3.
    duration = as.numeric( na_if( `PNQM Latency (ms)`, "null" ) ) * 1e6, #    4.
    type = as.factor( `Msg Type` ) #                                          5.
  ) %>%
  
  group_by( t ) %>%
  mutate(
    duplicate_count = n() - 1, #                                              6.
    duplicate_number = row_number() - 1
  ) %>%
  select( t, duration, type, duplicate_count, duplicate_number ) %>%
  ungroup() %>%
  arrange( t ) %>%
  
  mutate(
    N = row_number(), #                                                       7.
    arrival_T = t + duplicate_number, #                                       8.
    delta_t = arrival_T - lag(arrival_T), #                                   9.
    departure_T = arrival_T + duration #                                     10.
  ) %>%
  select( N, arrival_T, delta_t, type, duration, departure_T ) # 11.
```

Successfully loaded `r nrow( event_data )` observations from the first `r time_limit` nanoseconds of our event data.

# Calculate Number of Concurrent Events

- Create temporary lists of arrival and departure times for events; Add a step
size column of 1 and -1 respectively.
- Merge the arrival and departure tables together, order by time and then count
the cumulative sum of the assigned step size.
- Merge the concurrent event size data with the working event data on arrival
time so that we know the number of events in process when new events arrive

```{r concurrent_events_queue}
.arrivals <- event_data %>%
  filter( !is.na( departure_T ), departure_T != 0 ) %>%
  mutate( t = arrival_T, step = 1 ) %>%
  select( t, step )

.departures <- event_data %>%
  filter( !is.na( departure_T ), departure_T != 0 ) %>%
  mutate( t = departure_T, step = -1 ) %>%
  select( t, step )

.concurrent_event_count <- bind_rows( .arrivals, .departures ) %>%
  arrange( t ) %>%
  mutate( count = cumsum( step ))

event_data <- event_data %>%
  left_join( .concurrent_event_count, by=c('arrival_T' = 't' ) ) %>%
  rename( 'concurrent_event_count' = 'count' ) %>% 
  select( -'step' ) 
```
